#!/usr/bin/env python3
"""
PowerAutomation MCPç»Ÿä¸€æµ‹è¯•æ‰§è¡Œå™¨

åŸºäºPowerAutomationæµ‹è¯•æ¡†æ¶æ ‡å‡†ï¼Œè‡ªåŠ¨å‘ç°å¹¶æ‰§è¡Œæ‰€æœ‰MCPæ¨¡å—çš„æµ‹è¯•ã€‚
æ”¯æŒå¹¶è¡Œæµ‹è¯•æ‰§è¡Œã€ç»¼åˆæŠ¥å‘Šç”Ÿæˆå’Œé”™è¯¯å¤„ç†ã€‚

ä½œè€…: Manus AI
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2025-06-17
"""

import os
import sys
import json
import asyncio
import unittest
import subprocess
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import importlib.util

class MCPTestExecutor:
    """MCPç»Ÿä¸€æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, project_root: str = "/opt/powerautomation"):
        self.project_root = Path(project_root)
        self.mcp_adapter_path = self.project_root / "mcp" / "adapter"
        self.mcp_workflow_path = self.project_root / "mcp" / "workflow"
        self.test_results = []
        self.execution_start_time = datetime.now()
        
    def discover_all_tests(self) -> List[Dict[str, Any]]:
        """å‘ç°æ‰€æœ‰æµ‹è¯•æ–‡ä»¶"""
        test_modules = []
        
        # å‘ç°é€‚é…å™¨æµ‹è¯•
        if self.mcp_adapter_path.exists():
            for module_dir in self.mcp_adapter_path.iterdir():
                if module_dir.is_dir() and not module_dir.name.startswith('.'):
                    test_modules.extend(self._discover_module_tests(module_dir, 'adapter'))
        
        # å‘ç°å·¥ä½œæµæµ‹è¯•
        if self.mcp_workflow_path.exists():
            for module_dir in self.mcp_workflow_path.iterdir():
                if module_dir.is_dir() and not module_dir.name.startswith('.'):
                    test_modules.extend(self._discover_module_tests(module_dir, 'workflow'))
        
        print(f"âœ… å‘ç° {len(test_modules)} ä¸ªæµ‹è¯•æ¨¡å—")
        return test_modules
    
    def _discover_module_tests(self, module_dir: Path, module_type: str) -> List[Dict[str, Any]]:
        """å‘ç°å•ä¸ªæ¨¡å—çš„æµ‹è¯•"""
        tests = []
        
        # æŸ¥æ‰¾å•å…ƒæµ‹è¯•
        unit_tests_dir = module_dir / 'unit_tests'
        if unit_tests_dir.exists():
            for test_file in unit_tests_dir.glob('test_*.py'):
                tests.append({
                    'module_name': module_dir.name,
                    'module_type': module_type,
                    'test_type': 'unit',
                    'test_file': test_file,
                    'test_path': str(test_file.relative_to(self.project_root))
                })
        
        # æŸ¥æ‰¾é›†æˆæµ‹è¯•
        integration_tests_dir = module_dir / 'integration_tests'
        if integration_tests_dir.exists():
            for test_file in integration_tests_dir.glob('test_*.py'):
                tests.append({
                    'module_name': module_dir.name,
                    'module_type': module_type,
                    'test_type': 'integration',
                    'test_file': test_file,
                    'test_path': str(test_file.relative_to(self.project_root))
                })
        
        return tests
    
    def execute_single_test(self, test_info: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
        test_start_time = datetime.now()
        module_name = test_info['module_name']
        test_type = test_info['test_type']
        test_file = test_info['test_file']
        
        print(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {module_name} ({test_type})")
        
        try:
            # æ„å»ºæµ‹è¯•å‘½ä»¤
            cmd = [
                sys.executable, '-m', 'unittest', 
                f"{test_info['test_path'].replace('/', '.').replace('.py', '')}"
            ]
            
            # æ‰§è¡Œæµ‹è¯•
            result = subprocess.run(
                cmd,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            test_end_time = datetime.now()
            test_duration = (test_end_time - test_start_time).total_seconds()
            
            # è§£ææµ‹è¯•ç»“æœ
            success = result.returncode == 0
            
            test_result = {
                'module_name': module_name,
                'test_type': test_type,
                'test_file': test_file.name,
                'test_path': test_info['test_path'],
                'status': 'PASS' if success else 'FAIL',
                'duration': test_duration,
                'start_time': test_start_time.isoformat(),
                'end_time': test_end_time.isoformat(),
                'stdout': result.stdout,
                'stderr': result.stderr,
                'return_code': result.returncode
            }
            
            if success:
                print(f"âœ… {module_name} ({test_type}) - é€šè¿‡ ({test_duration:.2f}s)")
            else:
                print(f"âŒ {module_name} ({test_type}) - å¤±è´¥ ({test_duration:.2f}s)")
                if result.stderr:
                    print(f"   é”™è¯¯: {result.stderr[:200]}...")
            
            return test_result
            
        except subprocess.TimeoutExpired:
            test_result = {
                'module_name': module_name,
                'test_type': test_type,
                'test_file': test_file.name,
                'test_path': test_info['test_path'],
                'status': 'TIMEOUT',
                'duration': 300,
                'start_time': test_start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'stdout': '',
                'stderr': 'Test execution timeout (300s)',
                'return_code': -1
            }
            print(f"â° {module_name} ({test_type}) - è¶…æ—¶")
            return test_result
            
        except Exception as e:
            test_result = {
                'module_name': module_name,
                'test_type': test_type,
                'test_file': test_file.name,
                'test_path': test_info['test_path'],
                'status': 'ERROR',
                'duration': 0,
                'start_time': test_start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'stdout': '',
                'stderr': str(e),
                'return_code': -2
            }
            print(f"ğŸ’¥ {module_name} ({test_type}) - é”™è¯¯: {e}")
            return test_result
    
    def execute_tests_parallel(self, test_modules: List[Dict[str, Any]], max_workers: int = 4) -> List[Dict[str, Any]]:
        """å¹¶è¡Œæ‰§è¡Œæµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œæµ‹è¯• (æœ€å¤§å¹¶å‘: {max_workers})")
        print("=" * 80)
        
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            future_to_test = {
                executor.submit(self.execute_single_test, test): test 
                for test in test_modules
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_test):
                test_info = future_to_test[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    error_result = {
                        'module_name': test_info['module_name'],
                        'test_type': test_info['test_type'],
                        'test_file': test_info['test_file'].name,
                        'test_path': test_info['test_path'],
                        'status': 'ERROR',
                        'duration': 0,
                        'start_time': datetime.now().isoformat(),
                        'end_time': datetime.now().isoformat(),
                        'stdout': '',
                        'stderr': f'Execution error: {str(e)}',
                        'return_code': -3
                    }
                    results.append(error_result)
                    print(f"ğŸ’¥ {test_info['module_name']} æ‰§è¡Œå¼‚å¸¸: {e}")
        
        return results
    
    def execute_tests_sequential(self, test_modules: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é¡ºåºæ‰§è¡Œæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹é¡ºåºæ‰§è¡Œæµ‹è¯•")
        print("=" * 80)
        
        results = []
        for test_info in test_modules:
            result = self.execute_single_test(test_info)
            results.append(result)
        
        return results
    
    def generate_comprehensive_report(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        execution_end_time = datetime.now()
        total_duration = (execution_end_time - self.execution_start_time).total_seconds()
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(test_results)
        passed_tests = len([r for r in test_results if r['status'] == 'PASS'])
        failed_tests = len([r for r in test_results if r['status'] == 'FAIL'])
        timeout_tests = len([r for r in test_results if r['status'] == 'TIMEOUT'])
        error_tests = len([r for r in test_results if r['status'] == 'ERROR'])
        
        # æŒ‰æ¨¡å—ç»Ÿè®¡
        module_stats = {}
        for result in test_results:
            module_name = result['module_name']
            if module_name not in module_stats:
                module_stats[module_name] = {
                    'total': 0,
                    'passed': 0,
                    'failed': 0,
                    'timeout': 0,
                    'error': 0
                }
            
            module_stats[module_name]['total'] += 1
            if result['status'] == 'PASS':
                module_stats[module_name]['passed'] += 1
            elif result['status'] == 'FAIL':
                module_stats[module_name]['failed'] += 1
            elif result['status'] == 'TIMEOUT':
                module_stats[module_name]['timeout'] += 1
            elif result['status'] == 'ERROR':
                module_stats[module_name]['error'] += 1
        
        # æŒ‰æµ‹è¯•ç±»å‹ç»Ÿè®¡
        type_stats = {}
        for result in test_results:
            test_type = result['test_type']
            if test_type not in type_stats:
                type_stats[test_type] = {
                    'total': 0,
                    'passed': 0,
                    'failed': 0,
                    'timeout': 0,
                    'error': 0
                }
            
            type_stats[test_type]['total'] += 1
            if result['status'] == 'PASS':
                type_stats[test_type]['passed'] += 1
            elif result['status'] == 'FAIL':
                type_stats[test_type]['failed'] += 1
            elif result['status'] == 'TIMEOUT':
                type_stats[test_type]['timeout'] += 1
            elif result['status'] == 'ERROR':
                type_stats[test_type]['error'] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        comprehensive_report = {
            'test_execution_id': f'MCP_TestExecution_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'execution_summary': {
                'start_time': self.execution_start_time.isoformat(),
                'end_time': execution_end_time.isoformat(),
                'total_duration': total_duration,
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': failed_tests,
                'timeout_tests': timeout_tests,
                'error_tests': error_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0
            },
            'module_statistics': module_stats,
            'type_statistics': type_stats,
            'detailed_results': test_results,
            'failed_tests_summary': [
                {
                    'module': r['module_name'],
                    'test_type': r['test_type'],
                    'test_file': r['test_file'],
                    'status': r['status'],
                    'error': r['stderr'][:500] if r['stderr'] else 'No error message'
                }
                for r in test_results if r['status'] in ['FAIL', 'TIMEOUT', 'ERROR']
            ]
        }
        
        return comprehensive_report
    
    def save_report(self, report: Dict[str, Any]) -> Path:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        report_filename = f"mcp_comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path = self.project_root / "test" / report_filename
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report_path
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        summary = report['execution_summary']
        
        print("\\n" + "=" * 80)
        print("ğŸ“Š PowerAutomation MCPæµ‹è¯•æ‰§è¡Œæ‘˜è¦")
        print("=" * 80)
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {summary['total_duration']:.2f}ç§’")
        print(f"ğŸ“ æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"âœ… é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"âŒ å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"â° è¶…æ—¶æµ‹è¯•: {summary['timeout_tests']}")
        print(f"ğŸ’¥ é”™è¯¯æµ‹è¯•: {summary['error_tests']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        # æŒ‰æ¨¡å—æ˜¾ç¤ºç»“æœ
        print("\\nğŸ“¦ æ¨¡å—æµ‹è¯•ç»“æœ:")
        for module_name, stats in report['module_statistics'].items():
            status_icon = "âœ…" if stats['failed'] + stats['timeout'] + stats['error'] == 0 else "âŒ"
            print(f"  {status_icon} {module_name}: {stats['passed']}/{stats['total']} é€šè¿‡")
        
        # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
        if report['failed_tests_summary']:
            print("\\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for failed_test in report['failed_tests_summary']:
                print(f"  - {failed_test['module']} ({failed_test['test_type']}): {failed_test['status']}")
                if failed_test['error']:
                    print(f"    é”™è¯¯: {failed_test['error'][:100]}...")
    
    def run_all_tests(self, parallel: bool = True, max_workers: int = 4) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ PowerAutomation MCPç»Ÿä¸€æµ‹è¯•æ‰§è¡Œå™¨")
        print("=" * 80)
        
        # å‘ç°æ‰€æœ‰æµ‹è¯•
        test_modules = self.discover_all_tests()
        
        if not test_modules:
            print("âš ï¸  æ²¡æœ‰å‘ç°ä»»ä½•æµ‹è¯•æ–‡ä»¶")
            return False
        
        # æ‰§è¡Œæµ‹è¯•
        if parallel:
            test_results = self.execute_tests_parallel(test_modules, max_workers)
        else:
            test_results = self.execute_tests_sequential(test_modules)
        
        # ç”ŸæˆæŠ¥å‘Š
        comprehensive_report = self.generate_comprehensive_report(test_results)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.save_report(comprehensive_report)
        
        # æ‰“å°æ‘˜è¦
        self.print_summary(comprehensive_report)
        
        print(f"\\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # è¿”å›æ˜¯å¦æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡
        summary = comprehensive_report['execution_summary']
        all_passed = (summary['failed_tests'] + summary['timeout_tests'] + summary['error_tests']) == 0
        
        if all_passed:
            print("\\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†!")
        else:
            print("\\nâš ï¸  æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¯¦ç»†æŠ¥å‘Š")
        
        return all_passed

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='PowerAutomation MCPç»Ÿä¸€æµ‹è¯•æ‰§è¡Œå™¨')
    parser.add_argument('--sequential', action='store_true', help='é¡ºåºæ‰§è¡Œæµ‹è¯•ï¼ˆé»˜è®¤å¹¶è¡Œï¼‰')
    parser.add_argument('--workers', type=int, default=4, help='å¹¶è¡Œæ‰§è¡Œçš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--project-root', default='/opt/powerautomation', help='é¡¹ç›®æ ¹ç›®å½•')
    
    args = parser.parse_args()
    
    executor = MCPTestExecutor(args.project_root)
    success = executor.run_all_tests(
        parallel=not args.sequential,
        max_workers=args.workers
    )
    
    if not success:
        sys.exit(1)

if __name__ == '__main__':
    main()

