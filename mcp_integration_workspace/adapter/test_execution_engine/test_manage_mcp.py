#!/usr/bin/env python3
"""
Test Management MCP ä¸»æ¨¡å—
PowerAutomationæµ‹è¯•æ¡†æ¶ç®¡ç†å™¨ï¼Œæä¾›ç»Ÿä¸€çš„æµ‹è¯•ç”Ÿæˆã€æ‰§è¡Œå’Œç®¡ç†åŠŸèƒ½

åŸºäºPowerAutomation MCPæ ‡å‡†ï¼Œé›†æˆæµ‹è¯•æ¡†æ¶ç”Ÿæˆã€æ‰§è¡Œã€ä¿®å¤å’ŒæŠ¥å‘ŠåŠŸèƒ½ã€‚
æ”¯æŒè‡ªåŠ¨å‘ç°MCPæ¨¡å—ã€ç”Ÿæˆæ ‡å‡†åŒ–æµ‹è¯•ã€å¹¶è¡Œæ‰§è¡Œæµ‹è¯•ã€ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šã€‚

ä½œè€…: PowerAutomation Team
ç‰ˆæœ¬: 1.0.0
æ—¥æœŸ: 2025-06-17
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

from .framework.test_framework_generator import MCPTestFrameworkGenerator
from .framework.test_executor import MCPTestExecutor
from .framework.test_framework_fixer import MCPTestFrameworkFixer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TestManageMCP:
    """
    Test Management MCP ä¸»ç±»
    æä¾›PowerAutomationæµ‹è¯•æ¡†æ¶çš„ç»Ÿä¸€ç®¡ç†æ¥å£
    """
    
    def __init__(self, project_root: str = "/opt/powerautomation"):
        """
        åˆå§‹åŒ–Test Management MCP
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
        """
        self.project_root = Path(project_root)
        self.module_path = Path(__file__).parent
        self.reports_path = self.module_path / "reports"
        
        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
        self.reports_path.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.generator = MCPTestFrameworkGenerator(str(self.project_root))
        self.executor = MCPTestExecutor(str(self.project_root))
        self.fixer = MCPTestFrameworkFixer(str(self.project_root))
        
        # çŠ¶æ€ä¿¡æ¯
        self.last_generation_time = None
        self.last_execution_time = None
        self.last_fix_time = None
        
        logger.info(f"Test Management MCP initialized with project root: {self.project_root}")
    
    async def generate_test_frameworks(self) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ‰€æœ‰MCPæ¨¡å—çš„æµ‹è¯•æ¡†æ¶
        
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹ç”Ÿæˆæµ‹è¯•æ¡†æ¶...")
        
        try:
            results = self.generator.generate_all_test_frameworks()
            self.last_generation_time = datetime.now()
            
            # ä¿å­˜ç”ŸæˆæŠ¥å‘Š
            report_path = self.reports_path / f"generation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµ‹è¯•æ¡†æ¶ç”Ÿæˆå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {report_path}")
            return {
                "status": "success",
                "results": results,
                "report_path": str(report_path),
                "generation_time": self.last_generation_time.isoformat()
            }
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æ¡†æ¶ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "generation_time": datetime.now().isoformat()
            }
    
    async def execute_all_tests(self, parallel: bool = True, max_workers: int = 4) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        
        Args:
            parallel: æ˜¯å¦å¹¶è¡Œæ‰§è¡Œ
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            æ‰§è¡Œç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œæµ‹è¯• (å¹¶è¡Œ: {parallel}, æœ€å¤§å¹¶å‘: {max_workers})...")
        
        try:
            success = self.executor.run_all_tests(parallel=parallel, max_workers=max_workers)
            self.last_execution_time = datetime.now()
            
            # è·å–æœ€æ–°çš„æµ‹è¯•æŠ¥å‘Š
            report_files = list(self.project_root.glob("test/mcp_comprehensive_test_report_*.json"))
            if report_files:
                latest_report = max(report_files, key=lambda x: x.stat().st_mtime)
                # å¤åˆ¶åˆ°æˆ‘ä»¬çš„æŠ¥å‘Šç›®å½•
                target_path = self.reports_path / f"execution_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                target_path.write_text(latest_report.read_text())
                
                logger.info(f"æµ‹è¯•æ‰§è¡Œå®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {target_path}")
                return {
                    "status": "success" if success else "partial_failure",
                    "all_tests_passed": success,
                    "report_path": str(target_path),
                    "execution_time": self.last_execution_time.isoformat()
                }
            else:
                return {
                    "status": "error",
                    "error": "æœªæ‰¾åˆ°æµ‹è¯•æŠ¥å‘Š",
                    "execution_time": self.last_execution_time.isoformat()
                }
                
        except Exception as e:
            logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "execution_time": datetime.now().isoformat()
            }
    
    async def fix_test_frameworks(self) -> Dict[str, Any]:
        """
        ä¿®å¤æµ‹è¯•æ¡†æ¶ä¸­çš„é—®é¢˜
        
        Returns:
            ä¿®å¤ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹ä¿®å¤æµ‹è¯•æ¡†æ¶...")
        
        try:
            results = self.fixer.fix_all_test_files()
            self.last_fix_time = datetime.now()
            
            # ä¿å­˜ä¿®å¤æŠ¥å‘Š
            report_path = self.reports_path / f"fix_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµ‹è¯•æ¡†æ¶ä¿®å¤å®Œæˆï¼ŒæŠ¥å‘Šä¿å­˜è‡³: {report_path}")
            return {
                "status": "success",
                "results": results,
                "report_path": str(report_path),
                "fix_time": self.last_fix_time.isoformat()
            }
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æ¡†æ¶ä¿®å¤å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "fix_time": datetime.now().isoformat()
            }
    
    async def get_test_status(self) -> Dict[str, Any]:
        """
        è·å–æµ‹è¯•çŠ¶æ€æ¦‚è§ˆ
        
        Returns:
            æµ‹è¯•çŠ¶æ€å­—å…¸
        """
        try:
            # å‘ç°æ‰€æœ‰æµ‹è¯•æ¨¡å—
            test_modules = self.executor.discover_all_tests()
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_modules = len(set(module['module_name'] for module in test_modules))
            total_tests = len(test_modules)
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            unit_tests = len([m for m in test_modules if m['test_type'] == 'unit'])
            integration_tests = len([m for m in test_modules if m['test_type'] == 'integration'])
            
            # æŒ‰æ¨¡å—ç±»å‹ç»Ÿè®¡
            adapter_modules = len(set(m['module_name'] for m in test_modules if m['module_type'] == 'adapter'))
            workflow_modules = len(set(m['module_name'] for m in test_modules if m['module_type'] == 'workflow'))
            
            # è·å–æœ€æ–°æŠ¥å‘Š
            latest_reports = {
                "generation": None,
                "execution": None,
                "fix": None
            }
            
            for report_type in latest_reports.keys():
                report_files = list(self.reports_path.glob(f"{report_type}_report_*.json"))
                if report_files:
                    latest_file = max(report_files, key=lambda x: x.stat().st_mtime)
                    latest_reports[report_type] = {
                        "file": str(latest_file),
                        "time": datetime.fromtimestamp(latest_file.stat().st_mtime).isoformat()
                    }
            
            return {
                "status": "success",
                "overview": {
                    "total_modules": total_modules,
                    "total_tests": total_tests,
                    "unit_tests": unit_tests,
                    "integration_tests": integration_tests,
                    "adapter_modules": adapter_modules,
                    "workflow_modules": workflow_modules
                },
                "last_operations": {
                    "generation": self.last_generation_time.isoformat() if self.last_generation_time else None,
                    "execution": self.last_execution_time.isoformat() if self.last_execution_time else None,
                    "fix": self.last_fix_time.isoformat() if self.last_fix_time else None
                },
                "latest_reports": latest_reports,
                "query_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"è·å–æµ‹è¯•çŠ¶æ€å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "query_time": datetime.now().isoformat()
            }
    
    async def run_full_test_cycle(self, fix_first: bool = True, parallel: bool = True, max_workers: int = 4) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æµ‹è¯•å‘¨æœŸï¼šç”Ÿæˆ -> ä¿®å¤ -> æ‰§è¡Œ
        
        Args:
            fix_first: æ˜¯å¦å…ˆä¿®å¤ç°æœ‰é—®é¢˜
            parallel: æ˜¯å¦å¹¶è¡Œæ‰§è¡Œæµ‹è¯•
            max_workers: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            å®Œæ•´å‘¨æœŸç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´æµ‹è¯•å‘¨æœŸ...")
        cycle_start_time = datetime.now()
        
        results = {
            "cycle_start_time": cycle_start_time.isoformat(),
            "steps": []
        }
        
        try:
            # æ­¥éª¤1: ç”Ÿæˆæµ‹è¯•æ¡†æ¶
            logger.info("æ­¥éª¤1: ç”Ÿæˆæµ‹è¯•æ¡†æ¶")
            generation_result = await self.generate_test_frameworks()
            results["steps"].append({
                "step": "generation",
                "result": generation_result
            })
            
            if generation_result["status"] != "success":
                logger.error("æµ‹è¯•æ¡†æ¶ç”Ÿæˆå¤±è´¥ï¼Œç»ˆæ­¢å‘¨æœŸ")
                return results
            
            # æ­¥éª¤2: ä¿®å¤æµ‹è¯•æ¡†æ¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if fix_first:
                logger.info("æ­¥éª¤2: ä¿®å¤æµ‹è¯•æ¡†æ¶")
                fix_result = await self.fix_test_frameworks()
                results["steps"].append({
                    "step": "fix",
                    "result": fix_result
                })
                
                if fix_result["status"] != "success":
                    logger.warning("æµ‹è¯•æ¡†æ¶ä¿®å¤å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            # æ­¥éª¤3: æ‰§è¡Œæµ‹è¯•
            logger.info("æ­¥éª¤3: æ‰§è¡Œæµ‹è¯•")
            execution_result = await self.execute_all_tests(parallel=parallel, max_workers=max_workers)
            results["steps"].append({
                "step": "execution",
                "result": execution_result
            })
            
            # è®¡ç®—æ€»ä½“ç»“æœ
            cycle_end_time = datetime.now()
            cycle_duration = (cycle_end_time - cycle_start_time).total_seconds()
            
            all_success = all(step["result"]["status"] == "success" for step in results["steps"])
            
            results.update({
                "cycle_end_time": cycle_end_time.isoformat(),
                "cycle_duration": cycle_duration,
                "overall_status": "success" if all_success else "partial_failure",
                "summary": {
                    "total_steps": len(results["steps"]),
                    "successful_steps": len([s for s in results["steps"] if s["result"]["status"] == "success"]),
                    "failed_steps": len([s for s in results["steps"] if s["result"]["status"] != "success"])
                }
            })
            
            # ä¿å­˜å‘¨æœŸæŠ¥å‘Š
            cycle_report_path = self.reports_path / f"full_cycle_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(cycle_report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            results["cycle_report_path"] = str(cycle_report_path)
            
            logger.info(f"å®Œæ•´æµ‹è¯•å‘¨æœŸå®Œæˆï¼Œæ€»è€—æ—¶: {cycle_duration:.2f}ç§’")
            return results
            
        except Exception as e:
            logger.error(f"å®Œæ•´æµ‹è¯•å‘¨æœŸå¤±è´¥: {e}")
            results.update({
                "cycle_end_time": datetime.now().isoformat(),
                "overall_status": "error",
                "error": str(e)
            })
            return results

# å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¯æŒ
class AsyncTestManageMCP(TestManageMCP):
    """å¼‚æ­¥ç‰ˆæœ¬çš„Test Management MCP"""
    
    async def __aenter__(self):
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        logger.info("Test Management MCP session ended")

# ä¾¿æ·å‡½æ•°
async def create_test_manager(project_root: str = "/opt/powerautomation") -> TestManageMCP:
    """
    åˆ›å»ºTest Management MCPå®ä¾‹
    
    Args:
        project_root: é¡¹ç›®æ ¹ç›®å½•
        
    Returns:
        TestManageMCPå®ä¾‹
    """
    return TestManageMCP(project_root)

# ä¸»å‡½æ•°ç”¨äºç›´æ¥è¿è¡Œ
async def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºç›´æ¥è¿è¡Œæµ‹è¯•ç®¡ç†"""
    test_manager = await create_test_manager()
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•å‘¨æœŸ
    results = await test_manager.run_full_test_cycle()
    
    print("\\n" + "="*80)
    print("ğŸ‰ Test Management MCP æ‰§è¡Œå®Œæˆ")
    print("="*80)
    print(f"æ€»ä½“çŠ¶æ€: {results['overall_status']}")
    print(f"æ‰§è¡Œæ­¥éª¤: {results['summary']['successful_steps']}/{results['summary']['total_steps']} æˆåŠŸ")
    print(f"æ€»è€—æ—¶: {results.get('cycle_duration', 0):.2f}ç§’")
    
    if 'cycle_report_path' in results:
        print(f"è¯¦ç»†æŠ¥å‘Š: {results['cycle_report_path']}")

if __name__ == "__main__":
    asyncio.run(main())

